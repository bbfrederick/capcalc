#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:50 $
#       $Id: linfit,v 1.4 2016/06/14 12:04:50 frederic Exp $
#
from __future__ import print_function, division
import sys
import getopt
import string
import platform
import rapidtide.tide_funcs as tide
import os

import numpy as np
from pylab import *
import nibabel as nib

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.semi_supervised import LabelPropagation
from sklearn.tree import DecisionTreeClassifier
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
import scipy.sparse as ss
from matplotlib import pyplot as plt

def usage():
    print("usage: supercluster datafile classes outputroot")
    print("")
    print("required arguments:")
    print("    datafile      - the name of the 4 dimensional nifti file to cluster")
    print("    classes       - the name of the 3 dimensional nifti file of the class labels")
    print("    outputroot    - the root name of the output nifti files")
    print("")
    print("optional arguments:")
    print("    --dmask=DATAMASK            - use DATAMASK to specify which voxels in the data to classify")
    print('    --n_estimators=ESIMATORS    - set ESTIMATORS (default is 100)')
    print('    --max_features=FEATURES     - set FEATURES (default is -1 - set equal to sqrt(n_estimators)')
    print('    --n_neighbors=NEIGHBORS     - use NEIGHBORS (default is 5)')
    print('    --type=CLASSIFIERTYPE       - set the classifier type (options are randomforest, knn,')
    print('                                  gradientboost, adaboost, extratrees. Default is randomforest)')
    print('    --weights=WEIGHTS           - use WEIGHTS weighting for knn (options are uniform and distance (default))')
    print('    --display                   - display feature importances if available')
    print('    --prescale                  - prescale input data prior to clustering')
    print('    --scaler=SCALER             - set scaling type (options are standard and robust (default))')
    print('    --bagging                   - construct a bagging classifier')
    print('    --scaleintervals=R1,R2,...  - apply scaling to ranges R1, R2,... independently')
    print('                                  NOTE: turns on prescaling')
    print("")
    return()


# set default variable values
usedmask = False
classifiertype = 'randomforest'
display = False
prescale = False
multirun = False
scaler = 'robust'
numtests = 10
intlist = None
bagging = True

# random forest variables
n_estimators = 100
max_features = -1

# knn variables
weights = 'distance'
n_neighbors = 5

# parse command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv, 'h', ["help",
                                                    "scaleintervals=",
                                                    "display",
                                                    "prescale",
                                                    "dmask=",
                                                    "numtests=",
                                                    "multirun",
                                                    "weights=",
                                                    "scaler=",
                                                    "n_estimators=",
                                                    "max_features=",
                                                    "n_neighbors=",
                                                    "type="])
except getopt.GetoptError as err:
    # print(help information and exit:
    print(str(err)) # will print something like "option -a not recognized"
    usage()
    sys.exit(2)

# handle required args first
if len(args) < 4:
    print('spatial fit has 3 required arguments - ', len(args) - 1, 'found')
    usage()
    sys.exit()

datafilename=args[1]
classfilename=args[2]
outputrootname=args[3]

for o, a in opts:
    if o == "--n_estimators":
        n_estimators = int(a)
        print('setting n_estimators to ', n_estimators)
    elif o == "--max_features":
        max_features = int(a)
        print('setting max_features to ', max_features)
    elif o == "--n_neighbors":
        n_neighbors = int(a)
        print('will use n_neighbors of', n_neighbors)
    elif o == "--dmask":
        usedmask = True
        datamaskname = a
        print('using', datamaskname, 'as data mask')
    elif o == "--scaler":
        scaler = a
        if scaler != 'robust' and scaler != 'standard':
            print('illegal scaling')
            sys.exit()
        print('will use ', scaler, 'scaling')
    elif o == "--weights":
        weights = a
        if weights != 'distance' and weights != 'uniform':
            print('illegal weighting')
            sys.exit()
        print('will use ', weights, 'weighting')
    elif o == "--numtests":
         numtests = int(a)
         print('will run', numtests, 'accuracy checks')
    elif o == "--multirun":
        multirun = True
        print('will run multiplle classifications')
    elif o == "--prescale":
        prescale = True
        print('will prescale data')
    elif o == "--scaleintervals":
        prescale = True
        intlist = list(map(int, a.split(',')))
        print('will use intervals', intlist)
        print('will use linkage', linkage)
    elif o == "--type":
        classifiertype = a
        if classifiertype != 'randomforest' and \
            classifiertype != 'knn' and \
            classifiertype != 'extratrees' and \
            classifiertype != 'adaboost' and \
            classifiertype != 'gradientboost':
            print('illegal classifier mode - must be randomforest, extratrees, knn, adaboost, or gradientboost')
            sys.exit()
    elif o in ("-h", "--help"):
        usage()
        sys.exit()
    else:
        assert False, "unhandled option"


print('Will perform', classifiertype, 'classification')
    
# read in data
print("reading in data arrays")
datafile_img, datafile_data, datafile_hdr, datafiledims, datafilesizes = tide.readfromnifti(datafilename)
classfile_img, classfile_data, classfile_hdr, classfiledims, classfilesizes = tide.readfromnifti(classfilename)
if usedmask:
    datamask_img, datamask_data, datamask_hdr, datamaskdims, datamasksizes = tide.readfromnifti(datamaskname)

xsize, ysize, numslices, timepoints = tide.parseniftidims(datafiledims)
xdim, ydim, slicethickness, tr = tide.parseniftisizes(datafilesizes)
    
# check dimensions
print("checking class dimensions")
if not tide.checkspacematch(datafiledims, classfiledims):
    print('input mask spatial dimensions do not match image')
    exit()
if not classfiledims[4] == 1:
    print('class file must have time dimension of 1')
    exit()

if usedmask:
    print("checking mask dimensions")
    if not tide.checkspacematch(datafiledims, datamaskdims):
        print('input mask spatial dimensions do not match image')
        exit()
    if not datamaskdims[4] == 1:
        print('input mask time must have time dimension of 1')
        exit()

# allocating arrays
print("reshaping arrays")
numspatiallocs = int(xsize) * int(ysize) * int(numslices)
rs_datafile = datafile_data.reshape((numspatiallocs, timepoints))
rs_classfile = classfile_data.reshape((numspatiallocs))
knownlocs = np.where(rs_classfile > 0)

print("masking arrays")
if usedmask:
    proclocs = np.where(datamask_data.reshape((numspatiallocs)) > 0.9)
else:
    datamaskdims = [1, xsize, ysize, numslices, 1]
    themaxes = np.max(rs_datafile, axis=1)
    themins = np.min(rs_datafile, axis=1)
    thediffs = (themaxes - themins).reshape((numspatiallocs))
    proclocs = np.where(thediffs > 0.0)

# construct the arrays 
y_known = rs_classfile[knownlocs]

X_known = rs_datafile[knownlocs, :][0]
X_predict = rs_datafile[proclocs, :][0]
print(rs_datafile.shape, X_known.shape, X_predict.shape, y_known.shape)
n_features = rs_datafile.shape[1]

print('data has', n_features, 'features')
if max_features == -1:
    max_features = int(np.ceil(np.sqrt(n_features)))

# Scale the data
if intlist is None:
    intlist = [timepoints]
if prescale:
    print('prescaling...')
    thepos = 0
    for interval in intlist:
        if scaler == 'standard':
            X_known[:, thepos:(thepos + interval)] = StandardScaler().fit_transform(X_known[:, thepos:(thepos + interval)])
            X_predict[:, thepos:(thepos + interval)] = StandardScaler().fit_transform(X_predict[:, thepos:(thepos + interval)])
        else:
            X_known[:, thepos:(thepos + interval)] = RobustScaler().fit_transform(X_known[:, thepos:(thepos + interval)])
            X_predict[:, thepos:(thepos + interval)] = RobustScaler().fit_transform(X_predict[:, thepos:(thepos + interval)])
        thepos += interval
    prescalename = 'prescale_' + scaler + '_'
    print('Done')
else:
    prescalename = ''

# take a look at it
if display and False:
    projection = TSNE().fit_transform(X_known)
    plt.scatter(*projection.T, **plot_kwds)
    plt.show()
    
# set up the classifier
if classifiertype == 'randomforest':
    if bagging:
        theclassifier = BaggingClassifier( \
            RandomForestClassifier( \
                n_estimators=n_estimators, \
                max_features=max_features, \
                max_depth=None,
                min_samples_split=2, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'randomforest_bagging_' + prescalename + str(n_estimators).zfill(3) + '_' + str(max_features).zfill(3)
    else:
        theclassifier = RandomForestClassifier( \
            n_estimators=n_estimators, \
            max_features=max_features, \
            max_depth=None,
            min_samples_split=2, \
            random_state=0, \
            n_jobs=-1)
        methodname = 'randomforest_' + prescalename + str(n_estimators).zfill(3) + '_' + str(max_features).zfill(3)

elif classifiertype == 'extratrees':
    if bagging:
        theclassifier = BaggingClassifier( \
            ExtraTreesClassifier( \
                n_estimators=n_estimators, \
                max_features=max_features, \
                max_depth=None,
                min_samples_split=2, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'extratrees_bagging_' + prescalename + str(n_estimators).zfill(3) + '_' + str(max_features).zfill(3)
    else:
        theclassifier = ExtraTreesClassifier( \
            n_estimators=n_estimators, \
            max_features=max_features, \
            max_depth=None,
            min_samples_split=2, \
            random_state=0, \
            n_jobs=-1)
        methodname = 'extratrees_' + prescalename + str(n_estimators).zfill(3) + '_' + str(max_features).zfill(3)

elif classifiertype == 'adaboost':
    theclassifier = AdaBoostClassifier(n_estimators=n_estimators)
    methodname = 'ada_' + prescalename + str(n_estimators).zfill(3)

elif classifiertype == 'gradientboost':
    theclassifier = GradientBoostingClassifier(n_estimators=n_estimators, max_features='auto')
    methodname = 'gbc_' + prescalename + str(n_estimators).zfill(3)

elif classifiertype == 'knn':
    if bagging:
        theclassifier = BaggingClassifier( \
            KNeighborsClassifier( \
                n_neighbors=n_neighbors, \
                weights=weights, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'knn_bagging_' + prescalename + weights + '_' + str(n_neighbors).zfill(2)
    else:
        theclassifier = KNeighborsClassifier( \
            n_neighbors=n_neighbors, \
            weights=weights, \
            n_jobs=-1)
        methodname = 'knn_' + prescalename + weights + '_' + str(n_neighbors).zfill(2)

else:
    print('illegal classifier type')
    sys.exit()

# check accuracy vs. number of components
ncomps = timepoints

# check the accuracy of the model
accuracies = []
if multirun:
    y_predict_all = np.zeros((X_predict.shape[0],numtests), dtype=np.float)
for testrun in range(numtests):
    X_train, X_test, y_train, y_test = train_test_split(X_known[:, 0:ncomps], y_known, test_size=0.2)
    theclassifier.fit(X_train, y_train)
   
    # now make predictions on the test data
    y_predict = theclassifier.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_predict))

    # if we are doing multirun, make a prediction on all known data for each training
    if multirun:
        print('predicting all data for test', testrun)
        y_predict_all[:, testrun] = theclassifier.predict(X_predict)
  
if len(accuracies) > 0:
    theaccuracies = np.asarray(accuracies, dtype=np.float)
    print('Accuracies for', ncomps, 'components (mean, std, min, max):', \
        np.mean(theaccuracies), np.std(theaccuracies), np.min(theaccuracies), np.max(theaccuracies))
    with open(outputrootname + '_' + methodname + '_accuracy.csv', "w") as text_file:
        dpath, dfile = os.path.split(datafilename)
        cpath, cfile = os.path.split(classfilename)
        fieldlist = [dfile, cfile, methodname, str(np.mean(theaccuracies)), str(np.std(theaccuracies)), str(np.min(theaccuracies)), str(np.max(theaccuracies))]
        text_file.write(','.join(fieldlist) + '\n')

# now classify all the known data to calculate metrics
y_predict_known = theclassifier.predict(X_known)
print('prediction of known locations complete')
confusion = confusion_matrix(y_known, y_predict_known)
classreport = classification_report(y_known, y_predict_known)
tide.writenpvecs(confusion, outputrootname + '_' + methodname + '_predict_all_confusion_matrix.txt')
print(classreport)

if not multirun:
    # now classify the entire image
    y_predict_all = theclassifier.predict(X_predict)
    print('prediction of all locations complete')

# run other classifier-specific metrics
if display:
    if classifiertype == 'randomforest' or classifiertype == 'extratrees' or classifiertype == 'adaboost' or classifiertype == 'gradientboost':
        plt.plot(theclassifier.feature_importances_)
        plt.show()

# save the data
theheader = datafile_hdr
theheader['dim'][4] = 1

# output classification of all known data
tempout = np.zeros((numspatiallocs), dtype='float')
tempout[knownlocs] = y_predict_known[:]
tide.savetonifti(tempout.reshape((xsize, ysize, numslices, 1)), datafile_hdr, datafilesizes,
    outputrootname + '_' + methodname + '_predict_known')

# save the command line
tide.writevec([' '.join(sys.argv)], outputrootname + '_' + methodname + '_commandline.txt')

# output classification of all known data
if multirun:
    tempout = np.zeros((numspatiallocs, numtests), dtype='float')
    tempout[proclocs, :] = y_predict_all[:, :]
    theheader = datafile_hdr
    theheader['dim'][4] = numtests
    tide.savetonifti(tempout.reshape((xsize, ysize, numslices, numtests)), theheader, datafilesizes,
        outputrootname + '_' + methodname + '_predict_all')
else:
    tempout = np.zeros((numspatiallocs), dtype='float')
    tempout[proclocs] = y_predict_all[:]
    tide.savetonifti(tempout.reshape((xsize, ysize, numslices, 1)), datafile_hdr, datafilesizes,
        outputrootname + '_' + methodname + '_predict_all')
